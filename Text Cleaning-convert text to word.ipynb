{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Text data requires special preparation before you can start using it for predictive modeling. The\n",
    "text must be parsed to remove words, called tokenization. Then the words need to be encoded\n",
    "as integers or oating point values for use as input to a machine learning algorithm, called\n",
    "feature extraction (or vectorization)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "There are some methed to do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Counts with CountVectorizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents\n",
    "and build a vocabulary of known words, but also to encode new documents using that vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown the fox jumped over the quick lazy dog brown Apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 8, 'quick': 7, 'brown': 1, 'fox': 3, 'jumped': 4, 'over': 6, 'lazy': 5, 'dog': 2, 'apple': 0}\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 2 1 1 1 1 1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"The quick brown fox jumped over the lazy dog.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bag-of-Words Model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The BOW model only considers if a known word occurs in a document or not. It does not care about meaning, context, and order in which they appear."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Steps for BOW\n",
    "1)Collect Data\n",
    "2)Design the Vocabulary\n",
    "3)Create Document Vectors\n",
    "4)Managing Vocabulary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Suppose this is the the text\n",
    "\n",
    "It was the great of times,\n",
    "it was the nice to meet,\n",
    "it is the age of AI,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocabulary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "it\n",
    "was\n",
    "the\n",
    "great\n",
    "of\n",
    "times\n",
    "nice\n",
    "to \n",
    "meet\n",
    "is\n",
    "age \n",
    "AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Vectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's turn each document of text into a vector that we can use as input or output for a machine learning model.Create for first Line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "it  1\n",
    "was 1\n",
    "the 1\n",
    "great  1\n",
    "of  1\n",
    "times  1\n",
    "nice  0\n",
    "to  0\n",
    "meet   0\n",
    "is  0\n",
    "age  0 \n",
    "AI   0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It was the great of times   [1,1,1,1,1,1,0,0,0,0,0,0]\n",
    "it was the nice to meet,    [1,1,1,0,0,0,1,1,1,0,0,0]\n",
    "it is the age of AI,        [1,0,1,0,1,0,0,0,0,0,1,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Limitations of BOW\n",
    "1 Semantic meaning: the basic BOW approach does not consider the meaning of the word in the document. It completely ignores the context in which itâ€™s used. The same word can be used in multiple places based on the context or nearby words.\n",
    "2 Vector size: For a large document, the vector size can be huge resulting in a lot of computation and time. You may need to ignore words based on relevance to your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Word counts are a good starting point, but are very basic. One issue with simple counts is that\n",
    "some words like the will appear many times and their large counts will not be very meaningful\n",
    "in the encoded vectors. An alternative is to calculate word frequencies, and by far the most\n",
    "popular method is called TF-IDF. This is an acronym that stands for Term Frequency - Inverse\n",
    "Document Frequency which are the components of the resulting scores assigned to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
